# ğŸ“˜ Batch ETL Pipeline into Amazon Redshift

## ğŸ¯ Objective
The goal of this project is to design and implement a batch ETL pipeline using AWS services.  
- Store raw sales data in **S3**  
- Transform data with **Glue ETL**  
- Catalog metadata in **Glue Catalog**  
- Load into **Amazon Redshift Serverless** for analytics  
- Manage all infrastructure with **Terraform**, without using the AWS Console  

---

## ğŸ›  AWS Services Used
- **Amazon S3** â†’ Raw & processed data storage  
- **AWS Glue** â†’ Data Catalog & ETL jobs  
- **Amazon Redshift Serverless** â†’ Data warehouse for queries  
- **AWS IAM** â†’ Roles and policies for secure access  
- **Terraform** â†’ Infrastructure as Code  
- **Python (Boto3, PySpark)** â†’ Data generation & ETL scripts  

---

## ğŸ“‚ Project Structure
```
batch_etl_pipeline/
â”‚â”€â”€ providers.tf       # Configures AWS provider & region
â”‚â”€â”€ variables.tf       # Input variables
â”‚â”€â”€ s3.tf              # Raw + processed S3 buckets
â”‚â”€â”€ iam.tf             # IAM roles/policies for Glue & Redshift
â”‚â”€â”€ glue.tf            # Glue database + catalog table
â”‚â”€â”€ glue_etl.tf        # Glue ETL job definition
â”‚â”€â”€ redshift.tf        # Redshift namespace & workgroup
â”‚â”€â”€ scripts/
â”‚    â”œâ”€â”€ gen_upload.py # Generates & uploads large CSV to S3
â”‚    â”œâ”€â”€ sales_etl.py  # Glue ETL script (CSV â†’ Parquet)
```

---

## ğŸ— Architecture
```
Data Generator (Python)
        â†“
     Amazon S3 (Raw CSV)
        â†“
 AWS Glue ETL (sales_etl.py)
        â†“
     Amazon S3 (Parquet)
        â†“
 Amazon Redshift Serverless
        â†“
       SQL Queries
```

---

## ğŸ“œ What Each File Does
- **providers.tf** â†’ Defines AWS provider and region  
- **variables.tf** â†’ Stores project variables (region, bucket names, etc.)  
- **s3.tf** â†’ Creates raw and processed S3 buckets  
- **iam.tf** â†’ IAM roles for Glue (S3 full access) & Redshift (S3 read access)  
- **glue.tf** â†’ Glue database (`salesdb`) and catalog table (`raw_sales`)  
- **glue_etl.tf** â†’ Glue ETL job (`sales-etl-job`) to transform CSV â†’ Parquet  
- **redshift.tf** â†’ Redshift namespace (`sales-namespace`) and workgroup (`sales-workgroup`)  
- **scripts/gen_upload.py** â†’ Generates large CSV (~4GB) and uploads to S3  
- **scripts/sales_etl.py** â†’ ETL script that converts CSV â†’ Parquet  

---

## ğŸ”„ Workflow (Step by Step)

1. **Provision Infrastructure with Terraform**
   ```bash
   terraform init
   terraform apply -auto-approve
   ```

2. **Generate and Upload Raw Data**
   ```bash
   python3 scripts/gen_upload.py
   ```

3. **Run Glue ETL Job**
   ```bash
   aws glue start-job-run --job-name sales-etl-job
   ```

4. **Load Data into Redshift**
   ```bash
   aws redshift-data execute-statement      --workgroup-name sales-workgroup      --database dev      --sql "COPY sales_raw FROM 's3://processed-sales-data-batch-etl/' IAM_ROLE '<redshift-role-arn>' FORMAT AS PARQUET;"
   ```

5. **Query Data in Redshift**
   ```bash
   aws redshift-data execute-statement      --workgroup-name sales-workgroup      --database dev      --sql "SELECT * FROM sales LIMIT 5;"
   ```

---

## â“ Why Not Use a Glue Crawler?
- Schema is **fixed and known** (`id, product, price, quantity, date`)  
- Defining tables directly in Terraform is **cheaper and reproducible**  
- Avoids manual setup in the console  

---

## âœ… Example Query Results
| id       | product | price | quantity | date       |
|----------|---------|-------|----------|------------|
| 48857866 | item    | 10.5  | 2        | 2025-08-17 |
| 48857867 | item    | 10.5  | 2        | 2025-08-17 |

---

## ğŸ“Œ Conclusion
This project demonstrates a **scalable, automated Batch ETL pipeline** built on AWS.  
It covers the full lifecycle: **Raw Data â†’ S3 â†’ Glue ETL â†’ Processed Data â†’ Redshift â†’ SQL Analytics**.  

- Infrastructure is fully managed with **Terraform**  
- Jobs and queries are executed via **AWS CLI**  
- No AWS Console was used  
- A reproducible, cost-efficient pipeline suitable for large datasets  
